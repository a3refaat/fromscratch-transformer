# FromScratch-Net ðŸ§ 

A fully functional **multi-layer perceptron (MLP)** framework implemented from scratch in NumPy â€” complete with forward propagation, backpropagation, batch training, weight initialization, and support for classification tasks like MNIST.
I plan to contribute to this architecture over time as a learning project. Long term goals involve support for image processing through convolutional layers, and eventually using this to build into RNNs and transformer architectures.

---

##  Features

- Layered architecture: Input, Hidden, Output, BatchNorm
- Activation functions: ReLU, Sigmoid, Softmax
- Loss functions: Cross-Entropy, MSE
- Batch training with gradient descent
- Optimizers: SGD, Momentum, Adam
- Learning Curve plots
- Modular design (clean, extensible Python classes)
- Glorot and He initialization
- Recursive backpropagation
- MNIST demo included
